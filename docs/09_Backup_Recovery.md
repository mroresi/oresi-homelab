# ðŸ§¯ ORESI // SYSTEMS â€” Section 9: Backup, Disaster Recovery & Documentation  
**Version:** 1.0  â€¢  **Date:** 2025-10-29  
**Applies to:** whitebox (.101), redbox (.100), bloodbox (.103), blackbox (.40)  
**Linked Systems:** Synology DSM (NFS), Tailscale Mesh  
**Objective:** Implement reliable, redundant, and automated backup systems that ensure full recoverability of VMs, containers, and configurations within 30 minutes of catastrophic failure.  

---

## 9.0 ðŸ’¡ Design Intent  
You donâ€™t have a â€œhomelabâ€ until you have backups that restore cleanly.  
This phase focuses on:
- ðŸ§± Immutable backup infrastructure (Synology + offsite).  
- ðŸ” Versioned snapshots for VMs and containers.  
- ðŸ“¦ GitOps documentation for reproducibility.  
- ðŸ§° Automation hooks for restore and validation.  
- â˜ï¸ Optional offsite sync to cloud storage (B2 / S3 / rclone).

---

## 9.1 ðŸ§© Backup Topology

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          ORESI // SYSTEMS â€” BACKUP & RECOVERY LAYOUT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         [ whitebox (.101) ]
           â”œâ”€ vzdump â†’ NFS (blackbox:/volume2/backups)
           â”œâ”€ rsync configs â†’ blackbox:/volume2/docker_data
           â”œâ”€ Git push â†’ github.com/oresi/homelab-configs.git
           â””â”€ Tailscale Mesh â†’ remote access

         [ redbox (.100) ]
           â”œâ”€ Daily media stack backup â†’ blackbox
           â”œâ”€ qBittorrent session autosave
           â””â”€ Restore via Dockge

         [ bloodbox (.103) ]
           â”œâ”€ Ollama model backup â†’ Synology
           â””â”€ restore.sh (auto-pull latest models)

         [ blackbox (.40) ]
           â”œâ”€ Synology Hyper Backup â†’ Backblaze B2 (offsite)
           â”œâ”€ SnapReplicate docker_data hourly
           â””â”€ Backup Integrity Monitor
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

## 9.2 ðŸ’¾ VM Backups â€” Proxmox vzdump  

### 1ï¸âƒ£ Create Backup Job (GUI)
**Proxmox â†’ Datacenter â†’ Backup â†’ Add**
- Node: all  
- Schedule: daily @ 3AM  
- Storage: `synology_backups`  
- Mode: snapshot  
- Compression: `zstd`  
- Max backups: `5`  

### 2ï¸âƒ£ Command Line (for quick check)
```bash
vzdump 101 102 103 --compress zstd --storage synology_backups --quiet 1
```

### 3ï¸âƒ£ Restore Example
```bash
pct restore 101 /mnt/pve/synology_backups/dump/vzdump-lxc-101-*.tar.zst --storage local-lvm
```

---

## 9.3 ðŸ“¦ Container Configuration Backups  

**Synced Daily from Docker hosts to Synology:**
```bash
rsync -avz --delete /srv/config/ blackbox:/volume2/docker_data/config/
rsync -avz --delete /srv/media/ blackbox:/volume2/docker_data/media/
```

Schedule with `cron`:
```bash
0 2 * * * /usr/bin/rsync -avz /srv/config/ blackbox:/volume2/docker_data/config/
```

ðŸ’¡ *Tip:* Use Healthchecks.io to track if the rsync job stops unexpectedly.

---

## 9.4 ðŸ—ƒï¸ GitOps Configuration Backups  

### 1ï¸âƒ£ Version your docker-compose structure:
```bash
cd /opt/stacks
git add .
git commit -m "Automated update - $(date +%F)"
git push origin main
```

### 2ï¸âƒ£ Create `.env.template` for sensitive variables  
Never push credentials â€” store them separately in your **Vault or iCloud Secure Notes**.

---

## 9.5 â˜ï¸ Offsite Backups (Optional but Recommended)

### Option A â€” Synology Hyper Backup to Backblaze B2
- Package Center â†’ Install **Hyper Backup**  
- Source: `/volume2/backups`  
- Destination: Backblaze B2  
- Schedule: nightly at 3:30 AM  
- Retention Policy: Smart versioning (30 days)  

### Option B â€” rclone on Proxmox
```bash
rclone sync /mnt/pve/synology_backups b2:oresi-backups --progress --transfers=8
```

ðŸ“¦ **Alternative Destinations:**  
Dropbox, Google Drive, or another remote Synology NAS.

---

## 9.6 ðŸ§° Restore Procedures

| Scenario | Action | Estimated Recovery Time |
|-----------|---------|------------------------|
| Lost container configs | `rsync -avz blackbox:/volume2/docker_data/config/ /srv/config/` | 5 min |
| Corrupted Docker volume | `docker compose down && docker compose up -d` | 2 min |
| VM corruption | `pct restore` from Synology NFS | 10â€“15 min |
| Synology failure | Hyper Backup â†’ Backblaze restore | 30â€“60 min |
| Total network failure | Rejoin nodes via Tailscale MagicDNS | 10 min |

---

## 9.7 ðŸ§  Disaster Simulation Checklist  
- [ ] Unplug Synology for 10 minutes â€” ensure local cache continues running.  
- [ ] Simulate full VM restore from vzdump file.  
- [ ] Test a manual `rsync` restore for `/srv/config/`.  
- [ ] Validate offsite recovery from B2 using rclone.  
- [ ] Verify Tailscale access persists post-reboot.  

---

## 9.8 ðŸ“˜ Documentation & Runbook  

**Every system change â†’ document it.**  
Keep your operational history versioned alongside your Docker configs.

ðŸ—‚ï¸ Suggested structure for your Obsidian vault:
```
ORESI_HOMELAB/
â”œâ”€â”€ 01_Vision_Overview.md
â”œâ”€â”€ 02_Hardware_Architecture.md
â”œâ”€â”€ 03_Networking_Tailscale.md
â”œâ”€â”€ 04_Virtualization_Proxmox.md
â”œâ”€â”€ 05_Storage_NFS.md
â”œâ”€â”€ 06_Deployment_Docker.md
â”œâ”€â”€ 07_Monitoring.md
â”œâ”€â”€ 08_Automation.md
â””â”€â”€ 09_Backup_Recovery.md
```

---

## 9.9 ðŸ§¾ Quick Command Cheat-Sheet  
```bash
# Check vzdump job results
grep vzdump /var/log/syslog | tail -n 10

# Restore container config
rsync -avz blackbox:/volume2/docker_data/config/ /srv/config/

# Test remote backup sync
rclone check /mnt/pve/synology_backups b2:oresi-backups

# Verify tail-scale connectivity
tailscale status
```

---

âœ… **End of Section 9 â€” Backup, Disaster Recovery & Documentation**  
**ORESI // SYSTEMS â€” PHASE I COMPLETE**  
> Your homelab is now: monitored, automated, and self-restoring.  
> From this point onward â€” youâ€™re running an enterprise-grade system, built to survive chaos.
